# CXR_ReFusE

## Setup

CXR_ReFusE makes use of multiple github repos. Make sure to include all of them under CXR_ReFusE directory. 

* [ifcc](https://github.com/ysmiura/ifcc)
    * download model_medrad_19k.tar.gz by running `resources/download.sh`
    * Merge `ifcc_CXR_ReFusE_code` with the original directory
* [CheXbert](https://github.com/stanfordmlgroup/CheXbert)
    * download the [pretrained weights](https://stanfordmedicine.box.com/s/c3stck6w6dol3h36grdc97xoydzxd7w9) as well 
* [CXR-RePaiR](https://github.com/rajpurkarlab/CXR-RePaiR)
* [CXR-Report-Metric](https://github.com/rajpurkarlab/CXR-Report-Metric)
   * Merge `CXR-Report-Metric_CXR_ReFusE_code` with the original directory

As we made multiple edits to the ALBEF directory, please refer to the ALBEF directory uploaded here instead of cloning a new one. 

Download the [zipfile](https://drive.google.com/file/d/1VW8q0b4Jh6Pj3crpFHTRC3mTCRUjI2zi/view?usp=sharing) containing our dataset and place them in the appropriate folders. 


## Data-preprocessing
Here we use CXR-RePaiR's data preprocessing steps.
Refer to "Data Preprocessing" in cxr-repair for more detail. 
We obtain `data/cxr.h5`, `data/mimic_train_impressions.csv`, `data/mimic_test_impressions.csv` from CXR-RePaiR.

Afterwards, run `remove_prior_refs.py` to remove references to priors from the report data stored in `data/`.

## Training

For pretraining & fine-tuning ALBEF, use `pretrain_script.sh` and ve_script.sh in `ALBEF` directory. 
For pretraining, use `data/mimic_train.json`
For finetuning, use `data/ve_train.json` that is generated by `data/generate_ve_train_file.py`. 
`Jaehwan_edits.txt` keeps track of the edits I made to the original ALBEF github codebase. 

## Inference
`inference.sh` first calls `ALBEF/CXR_ReFusE_pipeline.py` to use cosine-sim & ve scores to select k' reports. 
`inference.sh` then calls `ifcc/m2trans_nli_filter.py` to select k reports based on nli scores
"Report Impression" column contains the reports before applying the nli filter, 
"filtered" column contains the filtered reports. 
    
## Evaluation
For evaluating the generated reports, use CXR-Report-Metric. 
Note that the original paper computed the metric scores using a subset of the mimic test set. 
First activate the conda env for CXR-Report-Metric
Next use `prepare_df.py` to select the inferences for the corresponding 2,192 samples from our generation. 
Then use `test_metric.py` to generate the scores. 
Finally use `compute_avg_score.py` to find the average scores. 
